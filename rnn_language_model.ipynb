{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, a language model using Recurrent Neural Networks (RNNs) will be created (in Keras) and trained on news headline data. It will then be used to generate news headlines of our own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the required libraries and preprocess the training data for the model. We will use Keras with a Tensorflow backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers import Dense, SimpleRNN, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Reading in the training data; taking a smaller set to reduce training time\n",
    "with open(\"headlines.train\", 'r') as f:\n",
    "    headlines_train = f.readlines()[:100000]\n",
    "\n",
    "# Removing excess punctuation and newline\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "headlines_train = [regex.sub('', h.split(\"\\n\")[0]) for h in headlines_train]\n",
    "\n",
    "# Define the unk, start and stop tokens\n",
    "UNK_TOKEN = \"<UNK>\" # UNKNOWN - mapping rare words\n",
    "START_TOKEN = \"<START>\"\n",
    "STOP_TOKEN = \"<STOP>\"\n",
    "\n",
    "def count_unigrams(text, unigram_dict):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    :param unigram_dict: A dictionary containing unigrams as keys and their respective counts as values\n",
    "    \"\"\"\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(tokens)):\n",
    "        unigram = tokens[i]\n",
    "        if unigram not in unigram_dict:\n",
    "            unigram_dict[unigram] = 1\n",
    "        else:\n",
    "            unigram_dict[unigram] += 1\n",
    "\n",
    "min_freq = 3 # The minimum word frequency to be present in the vocabulary\n",
    "\n",
    "# The following are used to keep track of and remove infrequent words\n",
    "low_freq = set()\n",
    "all_words = {}\n",
    "\n",
    "def replace_text_train(text):\n",
    "    return \" \".join([UNK_TOKEN if t in low_freq else t for t in text.split()])\n",
    "\n",
    "# Finding all words with low frequency\n",
    "for h in headlines_train:\n",
    "    count_unigrams(h, all_words)\n",
    "for word, count in all_words.items():\n",
    "    if count <= min_freq:\n",
    "        low_freq.add(word)\n",
    "# Replacing low frequency words from training dataset with UNK\n",
    "headlines_train_clean = [replace_text_train(h) for h in headlines_train]\n",
    "\n",
    "# Build vocabulary and make a mapping from index to word for generation\n",
    "vocab = set([item for sublist in map(lambda x: x.split(\" \"), headlines_train_clean) for item in sublist])\n",
    "vocab.add(STOP_TOKEN)\n",
    "vocab_list = list(vocab)\n",
    "word_to_index = {vocab_list[i]: i for i in range(len(vocab_list))}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our RNN, we be first converting our text into GloVe word embeddings before giving it as input. We'll also need to define some parameters which will be used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in GloVe embeddings as save them as a dictionary\n",
    "with open(\"glove_embeddings.txt\", 'r') as f:\n",
    "    gloves = [t.split(\" \") for t in f.readlines()]\n",
    "    gloves_dict = {t[0]: np.array(t[1:]) for t in gloves}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to used for the batch generator and model\n",
    "vocab_size = len(index_to_word.keys())\n",
    "sent_len = max([len(h.split(\" \")) for h in headlines_train_clean]) + 1\n",
    "glove_dim = next(iter(gloves_dict.values())).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Batches\n",
    "First we'll need to turn the headlines into data samples (where each sample is an output word given the entire history of previous words in the headline). To do this, we will iterate through all the headlines and through each word within the headline to get (history, word) pairs as our inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for h in headlines_train_clean:\n",
    "    # Pad the text in the beginning with start tokens\n",
    "    text = [START_TOKEN for _ in range(sent_len)] + h.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(text) - sent_len):\n",
    "        data.append((text[i:i+sent_len], text[i+sent_len]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras allows batches of data to be fed into the RNN through a generator, so we'll make such a generator to process the data and package it nicely for the model to use during the training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the data generator and model\n",
    "batch_size = 512\n",
    "num_batches = -(-len(data) // batch_size)\n",
    "\n",
    "def sample_generator():\n",
    "    while True:\n",
    "        random.shuffle(data)\n",
    "        for i in range(num_batches):\n",
    "            batch_input = np.zeros((batch_size, sent_len, glove_dim))\n",
    "            batch_label = np.zeros((batch_size, vocab_size))\n",
    "            for j in range(batch_size):\n",
    "                idx = j + i*batch_size\n",
    "                history, word = data[j]\n",
    "                for k in range(len(history)):\n",
    "                    if history[k] in gloves_dict:\n",
    "                        batch_input[j,k,:] = gloves_dict[history[k]]\n",
    "                batch_label[j,word_to_index[word]] = 1\n",
    "            yield batch_input, batch_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 128)               54912     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13550)             1747950   \n",
      "=================================================================\n",
      "Total params: 1,802,862\n",
      "Trainable params: 1,802,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hidden_neurons = 128\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_neurons, input_shape=(sent_len, glove_dim)))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code defines a few functions to generate sentences from the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_headline():\n",
    "    end_sentence = False\n",
    "    sent = np.zeros((sent_len, glove_dim))\n",
    "    \n",
    "    generated = []\n",
    "    curr_len = 0\n",
    "    while not end_sentence:\n",
    "        sent_input = np.expand_dims(sent[-sent_len:sent.shape[0]], axis=0)\n",
    "        word_probs = model.predict(sent_input, verbose=0)\n",
    "        next_word = sample(np.squeeze(word_probs, axis=0))\n",
    "        if next_word == word_to_index[STOP_TOKEN] or curr_len == sent_len:\n",
    "            end_sentence = True\n",
    "            print(' '.join(generated))\n",
    "        else:\n",
    "            if index_to_word[next_word] in gloves_dict:\n",
    "                word_embeded = gloves_dict[index_to_word[next_word]]\n",
    "            else:\n",
    "                word_embeded = np.zeros(glove_dim)\n",
    "            sent = np.concatenate((sent, np.expand_dims(word_embeded, axis=0)), axis=0)\n",
    "            generated.append(index_to_word[next_word])\n",
    "            curr_len += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin training, let's first look at what kind of headlines an untrained RNN generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vandal revises tested seminar conservationist face tougher mates slaps cipriani brindabella springsteen midwest brawlers warrant\n",
      "jailed stewarts ekka kirby christie mossman bride jacobs firies honours banking 18pc bellied milan wmd\n",
      "putting shocking rust victorians halls direction alarms partnerships expense destruction an penguin nov passion pritchard\n",
      "share irrigator childrens adani abalone simulation plays opponents toodyay eureka tehran exmouth coldest tally stalked\n",
      "thousands donors scales abandon humane lnp preparation finalises alcopop ada audit martyn showing aplenty boer\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generate_headline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't make much sense, eh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Training\n",
    "Now that we've constructed our RNN, we can begin training. This takes quite a while (~30 minutes) to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1447/1447 [==============================] - 1735s 1s/step - loss: 0.7825\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "smokers <UNK> man injured service station redevelopment\n",
      "\n",
      "colombia court <UNK>\n",
      "\n",
      "govt announces 2m in melbourne first\n",
      "\n",
      "Epoch 2/3\n",
      "1447/1447 [==============================] - 1675s 1s/step - loss: 1.0325\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "thunderbirds systemic dairy hope new shire shire\n",
      "\n",
      "temporary north years palestinians in the bill cherry flowering american olive of mining\n",
      "\n",
      "company new shire shire\n",
      "\n",
      "Epoch 3/3\n",
      "1447/1447 [==============================] - 1642s 1s/step - loss: 1.2216\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "sharon to enrol super bowl burning phoenix\n",
      "\n",
      "france police investigate\n",
      "\n",
      "reveals an attempted on the island\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16462267048>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    for i in range(3):\n",
    "        generate_headline()\n",
    "        print()\n",
    "\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.fit_generator(sample_generator(), num_batches, 3,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've finally finished training our RNN! Let's see what kind of headlines we can generate now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first hand in the downgrades\n",
      "govt toughens urges progress\n",
      "more late over attempted to enrol campus\n",
      "researchers denmark of the <UNK> increases\n",
      "denmark us to enrol horne horne technology a\n",
      "bundaberg floods\n",
      "report more <UNK> <UNK> the increase\n",
      "<UNK> field <UNK> man forced following to\n",
      "bushrangers police investigate truck appearance\n",
      "<UNK> teen late late to blame burning after arrest\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    generate_headline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems much better! Although some of the headlines still don't make a lot of sense, they seem to follow the syntactic structure of a legitimate news headline. With more rigorous training and a more complex RNN architecture, we could probably do even better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
