{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling: Bigram\n",
    "\n",
    "In this project, a bigram language model will be implemented and trained on a dataset containing news headlines. We will then evaluate our model using the perplexity metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here we will be importing the necessary modules, as well as doing some basic preprocessing of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# Reading in the training and development datasets\n",
    "with open(\"headlines.train\", 'r') as f:\n",
    "    headlines_train = f.readlines()\n",
    "with open(\"headlines.dev\", 'r') as f:\n",
    "    headlines_dev = f.readlines()\n",
    "    \n",
    "# Removing excess punctuation and newline\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "headlines_train = [regex.sub('', h.split(\"\\n\")[0]) for h in headlines_train]\n",
    "headlines_dev = [regex.sub('', h.split(\"\\n\")[0]) for h in headlines_dev]\n",
    "\n",
    "# Defining UNK, START and STOP tokens\n",
    "UNK_TOKEN = \"<UNK>\" # UNKNOWN - mapping rare words\n",
    "START_TOKEN = \"<START>\"\n",
    "STOP_TOKEN = \"<STOP>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's look at what some of the headlines look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lockyer valley housing market bouncing back after\n",
      "\n",
      "fixed line death strangles telstra profits\n",
      "\n",
      "national rural news tuesday 6th november\n",
      "\n",
      "builders warned to control rubbish\n",
      "\n",
      "miners fall on china slowdown speculation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline in random.sample(headlines_train, 5):\n",
    "    print(headline)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unigrams(text, unigram_dict):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    :param unigram_dict: A dictionary containing unigrams as keys and their respective counts as values\n",
    "    \"\"\"\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(tokens)):\n",
    "        unigram = tokens[i]\n",
    "        if unigram not in unigram_dict:\n",
    "            unigram_dict[unigram] = 1\n",
    "        else:\n",
    "            unigram_dict[unigram] += 1\n",
    "\n",
    "def count_bigrams(text, bigram_dict):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    :param bigram_dict: A dictionary containing bigrams as keys and their respective counts as values\n",
    "    \"\"\"\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram = (tokens[i], tokens[i+1])\n",
    "        if bigram not in bigram_dict:\n",
    "            bigram_dict[bigram] = 1\n",
    "        else:\n",
    "            bigram_dict[bigram] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Probability\n",
    "Let's calculate the probability of seeing a word given the previous word, along with Laplace smoothing parameterized by `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(target, context, unigram_dict, bigram_dict, alpha, vocab_size):\n",
    "    \"\"\"\n",
    "    :param target: The word whose probability of seeing is being computed given the context\n",
    "    :param context: The word directly preceeding the target word\n",
    "    :param unigram_dict: A dictionary containing unigrams as keys and their respective counts as values\n",
    "    :param bigram_dict: A dictionary containing bigrams as keys and their respective counts as values\n",
    "    :param alpha: The amount of additive smoothing being applied\n",
    "    :param vocab_size: The size of the training vocabulary\n",
    "    :return: The probability of seeing the target word given the context\n",
    "    \"\"\"\n",
    "    bigram = bigram_dict[(context, target)] if (context, target) in bigram_dict else 0\n",
    "    return (bigram + alpha) / (unigram_dict[context] + vocab_size*alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sampling\n",
    "Once we can calculate the desired probabilities, we can now use that to sample words for generation. We will use it to sample a new word in accordance with its probability of following the previous word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(words, probs):\n",
    "    \"\"\"\n",
    "    :param words: The list of words to sample from\n",
    "    :param probs: The probabilities of seeing each word; probs[i] is the probability of seeing word[i]\n",
    "    :return: A word whose sampling likelihood is the probability of being seen given the context\n",
    "    \"\"\"\n",
    "    return words[np.argmax(np.random.multinomial(1, probs, 1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Headlines\n",
    "Now that we've all the key parts of our language model completed, let's see how well we can generate new headlines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0205 # optimum for lowest perplexity\n",
    "min_freq = 10 # The minimum word frequency to be present in the vocabulary\n",
    "\n",
    "# The following are used to keep track of and remove infrequent words\n",
    "low_freq = set()\n",
    "all_words = {}\n",
    "\n",
    "def generate_headline(unigram_dict, bigram_dict, alpha):\n",
    "    sent = [START_TOKEN]\n",
    "    while not sent[-1] == STOP_TOKEN:\n",
    "        words = list(vocab)\n",
    "        probs = [get_probability(word, sent[-1], unigram_dict, bigram_dict, alpha, len(vocab)) for word in words]\n",
    "        next_word = sample_word(words, probs)\n",
    "        sent.append(next_word)\n",
    "    print(' '.join(sent[1:-1]))\n",
    "\n",
    "def replace_text_train(text):\n",
    "    return \" \".join([UNK_TOKEN if t in low_freq else t for t in text.split()])\n",
    "\n",
    "def replace_text_dev(text):\n",
    "    return \" \".join([UNK_TOKEN if t not in vocab else t for t in text.split()])\n",
    "\n",
    "# Finding all words with low frequency\n",
    "for h in headlines_train:\n",
    "    count_unigrams(h, all_words)\n",
    "for word, count in all_words.items():\n",
    "    if count <= min_freq:\n",
    "        low_freq.add(word)\n",
    "# Replacing low frequency words from training dataset with UNK\n",
    "headlines_train_clean = [replace_text_train(h) for h in headlines_train]\n",
    "\n",
    "# Initialize unigram and bigram dictionaries\n",
    "unigrams = {}\n",
    "bigrams = {}\n",
    "\n",
    "# Generating unigram and bigram dictionaries\n",
    "for h in headlines_train_clean:\n",
    "    count_unigrams(h, unigrams)\n",
    "    count_bigrams(h, bigrams)\n",
    "\n",
    "# Creating the training vocabulary\n",
    "vocab = set([item for sublist in map(lambda x: x.split(\" \"), headlines_train_clean) for item in sublist])\n",
    "vocab.add(START_TOKEN)\n",
    "vocab.add(STOP_TOKEN)\n",
    "\n",
    "# Replacing unseen vocabulary from development dataset with UNK\n",
    "headlines_dev_clean = [replace_text_dev(h) for h in headlines_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrolments diesel tamar prior yahoo mile manchester city\n",
      "\n",
      "brisbane parade to take joint shelves dams\n",
      "\n",
      "<UNK> safe in wake health australia cotton taswater uproar runaway apprenticeships usual arrested\n",
      "\n",
      "lord edward comp christensen confirmed bionic compensation to <UNK> shooting study calls kerry locusts easier under bowlers bats alkatiri briefed changed deaths\n",
      "\n",
      "<UNK> planning maldives female <UNK> replacement points\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    generate_headline(unigrams, bigrams, alpha)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation: Perplexity\n",
    "We can see how well our model does when encountering text from an unseen development set by calculating the perplexity. The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of words. The higher the conditional probability of the word sequence, the lower the perplexity (which is what we aim for, on the test set). \n",
    "\n",
    "For a bigram model trained on a test set W containing N words:\n",
    "$$ \\begin{equation*}\n",
    "    PP(W) = \\sqrt[N]{ \\prod_{i=1}^{N}  \\frac{1}{P( w_{i} |  w_{i-1})}  } \n",
    "\\end{equation*} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text, unigram_dict, bigram_dict, alpha, vocab_size):\n",
    "    \"\"\"\n",
    "    :param text: A headline, consisting of a string of words\n",
    "    \"\"\"\n",
    "    tokens = [START_TOKEN] + text.split(\" \") + [STOP_TOKEN]\n",
    "    log_sum = 0\n",
    "    for i in range(len(tokens) - 1):\n",
    "        prob = get_probability(tokens[i+1], tokens[i], unigram_dict, bigram_dict, alpha, vocab_size)\n",
    "        log_sum += np.log(prob)*(-1) # using log sum to prevent integer underflow\n",
    "\n",
    "    return np.exp(log_sum / (len(tokens) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average perplexity on dev set: 782.77\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "for h in headlines_dev_clean:\n",
    "    perp = calculate_perplexity(h, unigrams, bigrams, alpha, len(vocab))\n",
    "    perplexities.append(perp)\n",
    "print(\"Average perplexity on dev set: %.02f\" % (sum(perplexities) / len(perplexities)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
